---
title: "Homework 3"
author: "Henry Stoddard"
date: "10/10/2020"
output: github_document
---
## Problem 0
Creating initial markdown file and rendering to github doc

## Problem 1
```{r}
library(tidyverse)
library(p8105.datasets)
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.
Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables - name, aisle, department, and some numeric codes.

How many aisles and which are most items from?

```{r}
aisles_df = instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
There are `r nrow(aisles_df)` aisles. The most items are ordered from fresh vegetables and fresh fruits.

Making a plot that shows number of items ordered in each aisle for aisles with more than 10,000 items ordered!
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table!!
This table shows the three most popular items in the baking ingredients, dog food care, and packaged vegetables/fruits aisles, as well as how many times each item was ordered.
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Apples vs. Ice cream table
This table shows the mean hour of the day at which pink lady apples and coffee ice cream were ordered by day of the week.
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

## Problem 2

Loading and tidying data

```{r}
accel_df = read.csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440, names_to = "minute",
               names_prefix = "activity_", values_to = "activity_count") %>% 
  mutate(day = factor(day),
         minute = as.numeric(minute),
         weekday_weekend = recode(day, "Monday" = "weekday",
                                  "Tuesday" = "weekday",
                                  "Wednesday" = "weekday",
                                  "Thursday" = "weekday",
                                  "Friday" = "weekday",
                                  "Saturday" = "weekend",
                                  "Sunday" = "weekend"))
```
This data set is `r nrow(accel_df)` rows long and `r ncol(accel_df)` columns wide. It contains variables on week, day of the week, minute of the day, activity count by minute, and whether the observation is from a weekday or weekend. Day is a factor variable and minute is a numeric variable.

Now making a table to show total activity by day of the week, in order from most to least activity.
```{r q2 p2}
accel_df %>% 
  group_by(day, day_id) %>%
  summarise(total_activity = sum(activity_count))%>%
  ungroup(day) %>% 
  mutate(rank = min_rank(desc(total_activity))) %>% 
  arrange(rank) %>% knitr::kable()
```

There are no obvious trends. It looks like the end of the week/ weekend days are more active, but it also looks like those days can be the least active, especially Saturday.

Now, I will create a plot that shows the 24 hour activity time courses for each day. Color indicates day of the week.
```{r q2 p3}
accel_df2 = accel_df %>% 
  mutate(day_id = as.factor(day_id)) %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day_id)) +
  geom_smooth(se = F) +
  labs(title = "activity count by minute of day by day of week") +
  theme(legend.position = "bottom")
```

There are no overtly obvious trends once again. Activity seems to be low in the early morning, spikes around 10am and then again around 8pm, especially on Fridays.

## Problem 3

```{r q3 import}
library(p8105.datasets)
data("ny_noaa")
ny_noaa %>% select(everything()) %>% summarise_all(funs(sum = sum(is.na(.))))
```
The ny_noaa dataset is `r nrow(ny_noaa)` rows long by `r ncol(ny_noaa)` columns wide. The variables include weather station, date of observation, precipitation, snow, snow depth, and min and max temp. Precipitation is in tenths of a millimeter, snow and snow depth are in millimeters, and tmax and tmin are in tenths of a degree celsius. ID and date have no missing values, but the other 5 variables have a lot of missing data.

```{r q3 p1}
noa_1 = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>% 
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax),
         tmax = tmax/10,
         tmin = as.numeric(tmin),
         tmin = tmin/10)
```
Now the noa_1 dataset has a column for year, month, and day. Precipitation is in millimeters, and temp min and max are in degrees Celsius (and they are numeric).
```{r common snow}
noa_1 %>% 
  count(snow) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  arrange(rank) %>% 
  knitr::kable()
```
The most commonly observed value for snowfall is 0 millimeters.

```{r q3 p2}
noa_1 %>% 
  group_by(id, year, month) %>% 
  filter(month = c(1, 2)) %>% 
  summarise(tmax)
```


```{r q3 p2}
noa_1_plot = 
  noa_1 %>%
  select(year, month, id, tmax) %>% 
  group_by(id, year, month) %>% 
  filter(month == c("1", "7")) %>% 
  mutate(month = recode(month, "1" = "January",
                        "7" = "July")) %>% 
  summarize(avg_tmax = mean(tmax, na.rm = T)) %>% 
  ggplot(aes(x = year, y = avg_tmax, group = id, color = id)) +
  geom_point() +
  geom_path()+
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = "none")+
  labs(title = "Average Max Temp of reporting stations over the years, january vs. july", x = "Year", y = "Avg Max Temp")
```
Again, not many trends. Looks like overall temps range in each month very within a 20 degree celsius range. The july average max is higher than the january average max.

```{r q3 p3}
noa_1_plot2 =
  noa_1 %>% 
  pivot_longer(tmax:tmin, names_to = "Observation",
               values_to = "Tempurature") %>% 
  ggplot(aes(x = Tempurature, fill = Observation))+
  geom_density(alpha = 0.5)
```

part 3 make 2 plots separate and then use patchwork to pull them together. first one should be contour/bin/or hex plot not scatter plot. second plot should be filter step, then showuse violin, box, or ridge plot showing one box per year